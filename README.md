# pong-with-policy-gradients

Policy Gradient methods rely on optimizing parameterized policies with respect to expected cumulative rewards using gradient descent. That is, they model and optimize the policy directly, not needing the value function. The game-play is simple: the agent receives an image frame and decides if it wants to move the paddle up or down (i.e. a binary choice). After every single choice, the game simulator executes the action and gives a reward: either a +1 reward if the ball went past the opponent, a -1 reward if the agent missed the ball, or 0 otherwise. The goal is to move the paddle such that the agent gets the most reward. This is an example of a classical reinforcement learning task where the image (ball and paddle locations) are the states and the actions are the aforementioned possible paddle movements.
